# üöÄ Configurer Ollama au lieu d'OpenAI

## Pourquoi Ollama ?

- ‚úÖ **Gratuit** : Pas besoin de cl√© API payante
- ‚úÖ **Local** : Fonctionne sur votre machine
- ‚úÖ **Open Source** : Mod√®les libres
- ‚úÖ **Compatible** : Spring AI supporte Ollama nativement

## üìã Pr√©requis

### √âtape 1 : Installer Ollama

1. **T√©l√©chargez Ollama** :
   - Windows : https://ollama.com/download/windows
   - Mac : https://ollama.com/download/mac
   - Linux : https://ollama.com/download/linux

2. **Installez Ollama** :
   - Ex√©cutez le fichier d'installation
   - Ollama d√©marrera automatiquement

3. **T√©l√©chargez un mod√®le** :
   ```powershell
   # Mod√®le recommand√© : Llama 3.2 (3B, rapide et efficace)
   ollama pull llama3.2
   
   # Ou pour un mod√®le plus puissant (mais plus lent)
   ollama pull llama3.1
   
   # Ou pour un mod√®le fran√ßais
   ollama pull mistral
   ```

### √âtape 2 : V√©rifier l'Installation

```powershell
# Tester Ollama
ollama run llama3.2 "Bonjour, comment √ßa va ?"
```

Si cela fonctionne, Ollama est correctement install√© !

## ‚öôÔ∏è Configuration de l'Application

### √âtape 1 : Mettre √† jour `application.properties`

Remplacez la configuration OpenAI par :

```properties
# Configuration Ollama
spring.ai.ollama.base-url=http://localhost:11434
spring.ai.ollama.chat.options.model=llama3.2
```

**Mod√®les disponibles** :
- `llama3.2` - Rapide et efficace (recommand√©)
- `llama3.1` - Plus puissant mais plus lent
- `mistral` - Bon pour le fran√ßais
- `phi3` - Tr√®s rapide, l√©ger

### √âtape 2 : Red√©marrer l'Application

1. **Arr√™tez Chatboot** si il est en cours d'ex√©cution
2. **D√©marrez Ollama** (il devrait √™tre d√©j√† d√©marr√©)
3. **Red√©marrez Chatboot**

## üß™ Tester

### Test 1 : V√©rifier qu'Ollama fonctionne

```powershell
# Dans un terminal
ollama list
```

Vous devriez voir les mod√®les t√©l√©charg√©s.

### Test 2 : Tester via l'API REST

```powershell
Invoke-WebRequest -Uri "http://localhost:8087/api/chat?query=Bonjour" | Select-Object -ExpandProperty Content
```

### Test 3 : Tester via Telegram

Envoyez un message √† votre bot Telegram. Il devrait r√©pondre !

## üîÑ Retour √† OpenAI (si n√©cessaire)

Si vous voulez revenir √† OpenAI plus tard :

1. **Remettez la d√©pendance OpenAI** dans `pom.xml`
2. **Commentez Ollama** dans `pom.xml`
3. **Mettez √† jour `application.properties`** :
   ```properties
   spring.ai.openai.api-key=VOTRE_CLE_OPENAI
   ```
4. **Red√©marrez l'application**

## üìù Configuration Compl√®te

### `application.properties` avec Ollama

```properties
spring.application.name=chatboot
server.port=8087

# Configuration Ollama
spring.ai.ollama.base-url=http://localhost:11434
spring.ai.ollama.chat.options.model=llama3.2

# Configuration MCP Client
spring.ai.mcp.client.streamable-http.connections.mcprh.url=http://localhost:8989/mcp

# Configuration Telegram
telegram.api.key=8571783033:AAHzmek6df009XwfImq_agqHVxcUDsHbTVU
```

## ‚ö†Ô∏è Notes Importantes

1. **Ollama doit √™tre d√©marr√©** avant Chatboot
2. **Le mod√®le doit √™tre t√©l√©charg√©** (`ollama pull llama3.2`)
3. **Ollama utilise plus de RAM** qu'OpenAI (mod√®les locaux)
4. **Les r√©ponses peuvent √™tre plus lentes** qu'OpenAI (selon votre machine)

## üéØ Avantages d'Ollama

- ‚úÖ Pas de limite de requ√™tes
- ‚úÖ Pas de co√ªts
- ‚úÖ Donn√©es restent locales (privacy)
- ‚úÖ Fonctionne hors ligne
- ‚úÖ Pas besoin de cl√© API

## üö® D√©pannage

### Erreur : "Connection refused"

**Solution** : Ollama n'est pas d√©marr√©. D√©marrez-le manuellement ou v√©rifiez qu'il est en cours d'ex√©cution.

### Erreur : "Model not found"

**Solution** : Le mod√®le n'est pas t√©l√©charg√©. Ex√©cutez :
```powershell
ollama pull llama3.2
```

### R√©ponses lentes

**Solution** : 
- Utilisez un mod√®le plus petit (`llama3.2` au lieu de `llama3.1`)
- Augmentez la RAM disponible
- Fermez d'autres applications

## üìö Ressources

- Site officiel : https://ollama.com
- Mod√®les disponibles : https://ollama.com/library
- Documentation Spring AI : https://docs.spring.io/spring-ai/reference/

